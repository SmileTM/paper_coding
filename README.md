# Paper Coding [TF2.0]

reimplement code from paper 

4. Albert

    [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](http://arxiv.org/abs/1909.11942)
    
     **[[code]](https://github.com/SmileTM/paper_coding/tree/master/ALBert)**
     
3. Bert

    [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](http://arxiv.org/abs/1810.04805)
    
    **[[code]](https://github.com/SmileTM/paper_coding/tree/master/Bert)**
    
2. Transformer_block

    [Attention Is All You Need](http://arxiv.org/abs/1706.03762)
    
    **[[code]](https://github.com/SmileTM/paper_coding/tree/master/Transformer)**

1. NMT_with_Attenion　

    [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5)　

    [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)

    **[[code]](https://github.com/SmileTM/paper_coding/tree/master/NMT_attention)**
    

 

    
